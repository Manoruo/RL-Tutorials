{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro into RL \n",
    "\n",
    "This is an introduction to RL concepts. RL makes use of a framework called Markov Decision Process (MDP). This framework is helpful when solving problems where you have to make sequential decisions and want to find the optimal decisions to maxmize a goal. In this notebook we implmenent a simple MDP and use the Value Iteration algorithm to find the most optimal action for our agent to take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the MDP\n",
    "states = [0, 1, 2]  # States\n",
    "actions = [0, 1]  # Actions: 0 - left, 1 - right\n",
    "transition_probabilities = {\n",
    "    0: {0: [(1.0, 0)], 1: [(1.0, 1)]},  # From state 0: action 0 -> state 0, action 1 -> state 1\n",
    "    1: {0: [(1.0, 0)], 1: [(1.0, 2)]},  # From state 1: action 0 -> state 0, action 1 -> state 2\n",
    "    2: {0: [(1.0, 2)], 1: [(1.0, 2)]}   # From state 2: action 0 -> state 2, action 1 -> state 2\n",
    "}\n",
    "rewards = {\n",
    "    0: {0: -1, 1: 0},  # From state 0: action 0 gives -1, action 1 gives 0\n",
    "    1: {0: 0, 1: 1},   # From state 1: action 0 gives 0, action 1 gives 1\n",
    "    2: {0: 0, 1: 0}    # From state 2: both actions give 0\n",
    "}\n",
    "gamma = 0.9  # Discount factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve MDP (using Value Iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration to find the optimal value function\n",
    "def value_iteration(states, actions, transition_probabilities, rewards, gamma, epsilon=1e-6):\n",
    "    V = np.zeros(len(states))  # Initialize value function\n",
    "    while True:\n",
    "        delta = 0  # To track the change in value function\n",
    "        for state in states:\n",
    "            v = V[state]\n",
    "            max_value = float('-inf')\n",
    "            for action in actions:\n",
    "                value = sum(p * (rewards[state][action] + gamma * V[next_state]) \n",
    "                            for p, next_state in transition_probabilities[state][action])\n",
    "                max_value = max(max_value, value)\n",
    "            V[state] = max_value\n",
    "            delta = max(delta, abs(v - V[state]))\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function: [0.9 1.  0. ]\n"
     ]
    }
   ],
   "source": [
    "# Compute the value function\n",
    "V_optimal = value_iteration(states, actions, transition_probabilities, rewards, gamma)\n",
    "print(\"Optimal Value Function:\", V_optimal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: Bellman equation tells us how good being at a particular state is. States closer to the highest reward (usually the goal) will have greater value.\n",
    "# you should display this information so it's clear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
