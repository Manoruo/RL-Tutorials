{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  gym \n",
    "import numpy as np \n",
    "from IPython.display import Video\n",
    "import os\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"MountainCar-v0\"\n",
    "PATH = f'Models/QLearn/{ENV_NAME}/'\n",
    "MODEL_FILE = os.path.join(PATH, ENV_NAME + \".pth\")\n",
    "os.makedirs(PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME, render_mode='rgb_array') # with human render mode you dont have to call env.render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we are doing below is taking a continous space and converting it to a discrete space by spliting up the space into intervals of fixed sizes\n",
    "BUCKET_SIZE = 20 # how many discrete spaces we want (i.e. intervals)\n",
    "NUM_OBSERVATIONS_PER_STATE = len(env.observation_space.high) # tells us how many \"observations\" are in our observation space (in this example we have 2, position and velocity)\n",
    "UNIQUE_OBSERVATION = [BUCKET_SIZE] * NUM_OBSERVATIONS_PER_STATE  # this tells us the size of each unique observation in the observation space (ex: position - 20, velocity - 20)\n",
    "observation_interval_size =  (env.observation_space.high - env.observation_space.low) / UNIQUE_OBSERVATION # how \"big\" each interval is\n",
    "\n",
    "ACTION_SPACE_SIZE = env.action_space.n # typically you have to know this before hand. This is the amount of actions your agent can take (in this example we can use .n to figure that out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20, 3)\n"
     ]
    }
   ],
   "source": [
    "# define the q table (we need a combination of each of the individual discrete spaces for each observation and the action space)\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(UNIQUE_OBSERVATION + [ACTION_SPACE_SIZE]))\n",
    "print(q_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_RATE = .95 # how much to \"trust\" future rewards\n",
    "LEARNING_RATE = .1 # how much to \"trust\" the action we took \n",
    "NUM_EPISODES = 25000 \n",
    "\n",
    "SAVE_VIDEO_EVERY = 5000\n",
    "mastered = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    discrete_space = (state - env.observation_space.low) / observation_interval_size # this finds the corresponding bucket for the given state\n",
    "    return tuple(discrete_space.astype(int)) # return as tuple so we can index into q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/RL/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 1/25000 | Cumlative Reward : -200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 5001/25000 | Cumlative Reward : -199.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 10001/25000 | Cumlative Reward : -113.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 15001/25000 | Cumlative Reward : -159.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 20001/25000 | Cumlative Reward : -114.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training \n",
    "for i in range(NUM_EPISODES):\n",
    "    render = False \n",
    "\n",
    "    if i % SAVE_VIDEO_EVERY == 0:\n",
    "        frames = []\n",
    "        render = True\n",
    "\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    discrete_state = get_discrete_state(state)\n",
    "    terminate = False \n",
    "    truncate = False \n",
    "    total_reward = 0\n",
    "\n",
    "    while not terminate and not truncate:\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "        new_state, reward, terminate, truncate, info = env.step(action)\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "        total_reward = total_reward + reward \n",
    "\n",
    "        max_future_q = np.max(q_table[new_discrete_state])\n",
    "        current_q = q_table[discrete_state + (action,)] # get the q_table for the observation combination we are at, then select the q value for the action that we took \n",
    "        \n",
    "        # Bellman Equation \n",
    "        new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT_RATE * max_future_q)\n",
    "\n",
    "        # update q_table \n",
    "        q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        # keep track of previous state \n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "        if render:\n",
    "            frames.append(env.render())\n",
    "    \n",
    "    if render:\n",
    "        print(f'Episode : {i + 1}/{NUM_EPISODES} | Cumlative Reward : {total_reward}')\n",
    "        video_path = os.path.join(PATH, f'{ENV_NAME}_train_video_ep_{i}.mp4')\n",
    "        imageio.mimsave(video_path, frames, fps=20)\n",
    "        \n",
    "    frames = []\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"Models/QLearn/MountainCar-v0/MountainCar-v0_train_video_ep_20000.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the last training example\n",
    "Video(video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
